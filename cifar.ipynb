{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>By: Joel <br> Class: DAAA/2B/06<br> admin no: 2112729</h1>\n",
    "<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar-100 \n",
    "> Collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.State of the art accuracy is at 96.808 as of 2022 \n",
    "\n",
    "Research on the dataset:\n",
    "> Cifar 100 contains 80 million tiny dataset images. They are colored which means they have 3 channels and each channel has 32 by 32 pixels.\n",
    "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks. \n",
    "\n",
    "|  SuperClass |  Classes |   \n",
    "|-------------|---|\n",
    "| aquatic mammals  |  \tbeaver, dolphin, otter, seal, whale |  \n",
    "|   fish    |  \taquarium fish, flatfish, ray, shark, trout | \n",
    "|  flowers |  \torchids, poppies, roses, sunflowers, tulips |\n",
    "|food containers|\tbottles, bowls, cans, cups, plates |\n",
    "|fruit and vegetables|apples, mushrooms, oranges, pears, sweet peppers|\n",
    "|household electrical devices|clock, computer keyboard, lamp, telephone, television|\n",
    "|household furniture|bed, chair, couch, table, wardrobe|\n",
    "|insects|\tbee, beetle, butterfly, caterpillar, cockroach|\n",
    "|large carnivores|\tbear, leopard, lion, tiger, wolf|\n",
    "|large man-made outdoor things|\tbridge, castle, house, road, skyscraper|\n",
    "|large natural outdoor scenes|\tcloud, forest, mountain, plain, sea|\n",
    "|large omnivores and herbivores|\tcamel, cattle, chimpanzee, elephant, kangaroo|\n",
    "| medium-sized mammals|fox, porcupine, possum, raccoon, skunk|\n",
    "| non-insect invertebrates|\tcrab, lobster, snail, spider, worm|\n",
    "|people|\tbaby, boy, girl, man, woman|\n",
    "|reptiles|\tcrocodile, dinosaur, lizard, snake, turtle|\n",
    "|small mammals|\thamster, mouse, rabbit, shrew, squirrel|\n",
    "|trees|\tmaple, oak, palm, pine, willow|\n",
    "|vehicles_1|\tbicycle, bus, motorcycle, pickup truck, train|\n",
    "|vehicles_2|\tlawn-mower, rocket, streetcar, tank, tractor|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar100\n",
    "from tensorflow.keras.layers import Conv2D,Flatten\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_cv\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key conclusions:\n",
    "- 500 data points per class in train \n",
    "- 400 data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
    "assert X_train.shape == (50000, 32, 32, 3)\n",
    "assert X_test.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 1)\n",
    "assert y_test.shape == (10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and val sets using train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape, \"y_test shape:\", y_test.shape)\n",
    "\n",
    "\n",
    "# print unique classes count\n",
    "print(\"Unique classes count:\", len(np.unique(y_train)))\n",
    "\n",
    "# print unique classes\n",
    "y_train_df = pd.DataFrame(y_train)\n",
    "print(y_train_df[0].value_counts())\n",
    "\n",
    "\n",
    "y_test_label = y_test\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)\n",
    "print('after',y_train.shape,y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape, \"y_test shape:\", y_test.shape)\n",
    "\n",
    "\n",
    "# print unique classes\n",
    "y_train_df = pd.DataFrame(y_train)\n",
    "print(y_train_df[0].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some images of the dataset\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train[i])\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for plotting\n",
    "import plotly\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "def create_trace(x,y,ylabel,color):\n",
    "        trace = go.Scatter(\n",
    "            x = x,y = y,\n",
    "            name=ylabel,\n",
    "            marker=dict(color=color),\n",
    "            mode = \"markers+lines\",\n",
    "            text=x\n",
    "        )\n",
    "        return trace\n",
    "    \n",
    "def plot_accuracy_and_loss(train_model):\n",
    "    hist = train_model.history\n",
    "    acc = hist['accuracy']\n",
    "    val_acc = hist['val_accuracy']\n",
    "    loss = hist['loss']\n",
    "    val_loss = hist['val_loss']\n",
    "    epochs = list(range(1,len(acc)+1))\n",
    "    \n",
    "    trace_ta = create_trace(epochs,acc,\"Training accuracy\", \"Green\")\n",
    "    trace_va = create_trace(epochs,val_acc,\"Validation accuracy\", \"Red\")\n",
    "    trace_tl = create_trace(epochs,loss,\"Training loss\", \"Blue\")\n",
    "    trace_vl = create_trace(epochs,val_loss,\"Validation loss\", \"Magenta\")\n",
    "   \n",
    "    fig = plotly.subplots.make_subplots(rows=1,cols=2, subplot_titles=('Training and validation accuracy',\n",
    "                                                             'Training and validation loss'))\n",
    "    fig.append_trace(trace_ta,1,1)\n",
    "    fig.append_trace(trace_va,1,1)\n",
    "    fig.append_trace(trace_tl,1,2)\n",
    "    fig.append_trace(trace_vl,1,2)\n",
    "    fig['layout']['xaxis'].update(title = 'Epoch')\n",
    "    fig['layout']['xaxis2'].update(title = 'Epoch')\n",
    "    fig['layout']['yaxis'].update(title = 'Accuracy', range=[0,1])\n",
    "    fig['layout']['yaxis2'].update(title = 'Loss', range=[0,1])\n",
    "\n",
    "    \n",
    "    iplot(fig, filename=f'accuracy-loss_{train_model}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Baseline\n",
    "> CIFAR 100 seems to be alot harder for a simple CNN model to recognize\n",
    "\n",
    "![](./image/Filtering.gif)\n",
    "> Image is different from a typical MNIST dataset as it is now coloured, below is a sneak peak of what goes on in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "epochs = 15\n",
    "num_classes = 100\n",
    "seed = np.random.seed(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(32,32,3)))\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='relu'))\n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "fashion_model.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "fashion_model.summary()\n",
    "\n",
    "h_callback = fashion_model.fit(X_train, y_train, epochs = 10,\n",
    "               validation_data=(X_val, y_val))\n",
    "\n",
    "# Plot train vs test loss during training \n",
    "plot_accuracy_and_loss(h_callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "> I definitely do not have enough datasets and will have to create more data by augmenting ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a random image\n",
    "plt.imshow(X_test[12])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use randaugment to augment the image\n",
    "fig = plt.figure(figsize=(32, 32))\n",
    "for i in range(16):\n",
    "    ax = fig.add_subplot(4,4,i+1)\n",
    "    augmented_image = keras_cv.layers.RandAugment(value_range=(0, 255), magnitude=0.01)(X_test[12])\n",
    "    plt.imshow(augmented_image.numpy().astype(\"int\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
    "assert X_train.shape == (50000, 32, 32, 3)\n",
    "assert X_test.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 1)\n",
    "assert y_test.shape == (10000, 1)\n",
    "\n",
    "# split into train and val sets using train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)\n",
    "plt.imshow(X_train[12])\n",
    "plt.show()\n",
    "\n",
    "# set up image augmentation\n",
    "(X_train_rotate, y_train_rotate), (X_test_rotate, y_test_rotate) = cifar100.load_data()\n",
    "\n",
    "X_train_rotate , X_val_rotate, y_train_rotate, y_val_rotate = train_test_split(X_train_rotate, y_train_rotate, test_size=0.2, random_state=42, stratify=y_train_rotate)\n",
    "\n",
    "\n",
    "\n",
    "print('before',X_train_rotate.shape,X_test_rotate.shape)\n",
    "print('before',y_train_rotate.shape,y_test_rotate.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_test_rotate_label = y_test_rotate\n",
    "y_train_rotate = to_categorical(y_train_rotate)\n",
    "y_test_rotate = to_categorical(y_test_rotate)\n",
    "y_val_rotate = to_categorical(y_val_rotate)\n",
    "\n",
    "# ImageDataGenerator rotation\n",
    "datagen = ImageDataGenerator(\n",
    "\tfeaturewise_center=False,\n",
    "    samplewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    samplewise_std_normalization=True,\n",
    "    zca_whitening=True,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=90,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    channel_shift_range=0.2,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "# iterator with batch size of 128 so its faster\n",
    "aug_iter = datagen.flow(X_train, batch_size=128)\n",
    "\n",
    "X_train_augmented = []\n",
    "# loop through and generate 1 image for each sample and append to the dataset\n",
    "for X_batch in aug_iter:\n",
    "    X_train_rotated = np.append(X_train_rotate, X_batch, axis=0)\n",
    "    if X_train_augmented.shape[0] == 160000:\n",
    "        break\n",
    "\n",
    "# concat the original and augmented data\n",
    "X_train = np.concatenate((X_train,X_train_augmented),axis=0)\n",
    "\t\n",
    "plt.imshow(X_train_rotated[12])\n",
    "plt.show()\n",
    "\n",
    "# merge rotated data with original\n",
    "# X_train = np.concatenate((X_train,X_train_rotate),axis=0)\n",
    "# X_val = np.concatenate((X_val,X_val_rotate),axis=0)\n",
    "# y_train = np.concatenate((y_train,y_train_rotate),axis=0)\n",
    "# y_val = np.concatenate((y_val,y_val_rotate),axis=0)\n",
    "# print('after',X_train.shape,X_test.shape)\n",
    "# print('after',y_train.shape,y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "### function for looping through all the models\n",
    "def tune_model_act_opt(act,opt):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                    activation=act,\n",
    "                    kernel_initializer='he_normal',\n",
    "                    input_shape=(28,28,1)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, (3, 3), activation=act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, (3, 3), activation=act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=act))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax'))  \n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(histories,act,opt):\n",
    "\t\t# plot loss\n",
    "\tpyplot.subplot(211)\n",
    "\tpyplot.title(f'Cross Entropy Loss {act},{opt}')\n",
    "\tpyplot.plot(histories.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(histories.history['val_loss'], color='orange', label='test')\n",
    "\t# plot accuracy\n",
    "\tpyplot.subplot(212)\n",
    "\tpyplot.title(f'Classification Accuracy {act},{opt}')\n",
    "\tpyplot.plot(histories.history['accuracy'], color='blue', label='train')\n",
    "\tpyplot.plot(histories.history['val_accuracy'], color='orange', label='test')\n",
    "\tpyplot.legend()\n",
    "\tpyplot.show()\n",
    "\n",
    "def run_test_harness_act_opt(act,opt,X_train, y_train, X_test, y_test,X_val,y_val):\n",
    "    model = tune_model_act_opt(act,opt)\n",
    "    # fit model\n",
    "    h_callback = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001)\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=128,validation_data=(X_val, y_val), verbose=0 ,callbacks=[h_callback,reduce_lr])\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "    summarize_diagnostics(history,act,opt)\n",
    "    return (acc*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "optimizers = ['adam', 'sgd', 'rmsprop', 'adagrad', 'adadelta', 'adamax', 'nadam']\n",
    "activation = ['relu', 'selu', 'elu', 'tanh',LeakyReLU(alpha=0.01)]\n",
    "scores_act_opt = list()\n",
    "activation_names = ['relu', 'selu', 'elu', 'tanh','LeakyReLU']\n",
    "act_opt = list(product(activation_names,optimizers))\n",
    "\n",
    "for act,opt in product(activation,optimizers):\n",
    "    acc = run_test_harness_act_opt(act,opt,X_train, y_train, X_test, y_test,X_val,y_val)\n",
    "    scores_act_opt.append(acc)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring on Cifar 10 (coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cifar 10\n",
    "(X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode=\"coarse\")\n",
    "assert X_train.shape == (50000, 32, 32, 3)\n",
    "assert X_test.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 1)\n",
    "assert y_test.shape == (10000, 1)\n",
    "\n",
    "# split train and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape, \"y_test shape:\", y_test.shape)\n",
    "\n",
    "# print unique classes count\n",
    "print(\"Unique classes count:\", len(np.unique(y_train)))\n",
    "\n",
    "# print number of samples in each class\n",
    "y_train_df = pd.DataFrame(y_train)\n",
    "print(y_train_df[0].value_counts())\n",
    "\n",
    "y_test_label = y_test\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)\n",
    "print('after',y_train.shape,y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "epochs = 15\n",
    "num_classes = 20\n",
    "seed = np.random.seed(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(20))\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "mc = ModelCheckpoint('dummy_model_20.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "h_callback = model.fit(X_train, y_train, epochs = 10,\n",
    "               validation_data=(X_val, y_val) , callbacks=[early_stopping, mc])\n",
    "\n",
    "# Plot train vs test loss during training \n",
    "plot_accuracy_and_loss(h_callback)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "146e877987a5ca10c5ba40ec3e7a25002bcba56c71dc1c36e6a7a19a2012e1b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
